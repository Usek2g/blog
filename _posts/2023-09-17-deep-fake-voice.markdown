---
layout: post
title:  "ディープフェイクによる音声なりすまし"
date:   2023-09-17 18:50:27 +0900
categories: activity
img: ai_deepfake_y.png
 # Add image post (optional)
tags: [Security] # add tag
description: サイバー情報共有イニシアティブ（J-CSIP） 運用状況にあった気になる話し
---

先週は電話番号を成りすましてかかってくる詐欺電話の話しをしました。おりしも自分の携帯電話にもIP電話の番号から詐欺電話がかかってきたところです(--;)
「電話番号がなりすまされても、声を聴けば本人かどうか分かるし騙されないぞ」と思う方もいるかもしれません。しかし現代のテクノロジーの進歩はすさまじいです。

IPAは、「サイバー情報共有イニシアティブ（J-CSIP） 運用状況 （2023年4月～6月）」を公開しました。J-CSIPは、IPAを情報ハブとして参加組織間で情報共有を行い、高度なサイバー攻撃対策につなげる取り組みです。レポートが掲載されているリンクは以下になります。

https://www.ipa.go.jp/security/j-csip/about.html

この中で目に留まったのが「電話を併用したビジネスメール詐欺（BEC）」。なんと専務に成りすました攻撃者が電話をかけてきたといいます。AIが生成する画像や動画が本物そっくりなことは皆さん既にご存じでしょうが、音声は画像や動画よりも見抜くのが難しいと思います。

セキュリティ企業オンカスペルスキーが音声のディープフェイク(「ディープラーニング（深層学習）」+「フェイク」）人工知能は、ここ数年で急速に発展し続けています。機械学習を使用すると、説得力のある画像、動画、または音声コンテンツの偽物を作成できます。)がどのように作られているのかを解説しています。

[日本だけではない 世界で増加する音声詐欺](https://blog.kaspersky.co.jp/audio-deepfake-technology/34254/)

驚いたのは単に声を真似するだけでなく、「どのように話すか」「何を言わなければならないか」「どのように言うべき」を踏まえて音声が生成されるということ。
Microsoftは2023年初めに、わずか3秒の音声サンプルを使って人間の声を再現できるアルゴリズムを発表しましたが、どんなに優れたテクノロジーも悪用の余地があります。

そしてこの記事にはさらに恐ろしい未来を示しています。「他人に成りすました音声としゃべりを再現できる」ということは「自分が成りすまされる」ということです。音声で成りすまされて銀行口座に登録された情報を変更されたら、恐ろしいことになります。まだまだ荒削りの分野のようですが、AIイラストでも分かるように機械学習分野は爆発的に進化しています。どう対策すればよいでしょうか。SNSでもそうですが、嘘つき・詐欺師はまず我々の感情を怒りや恐怖、焦りで揺れ動かし冷静な考えを指せないように仕掛けてきます。最終的には映像や声ではなく、「何を言っているのか」から判断するしかないのかもしれませんね。